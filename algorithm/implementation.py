# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
https://colab.research.google.com/drive/1wcVIe7Hj5siHQNjPKhlg1TvhJRQzw8St
"""

from nltk import pos_tag
import nltk
import numpy as np
from typing import List, Tuple

from model import Model

import torch.nn as nn
import numpy as np
import torch
import json

from torch import tensor

import torch
from torch.utils.data import Dataset, DataLoader

import torch.nn as nn
import torch.nn.functional as F

RD_SEED = 213142

torch.manual_seed(RD_SEED)
np.random.seed(RD_SEED)

torch.backends.cudnn.deterministic = True


nltk.download('averaged_perceptron_tagger')


def encode_tag(sentence, vocab):
  indices = torch.Tensor([]).to('cpu')
  for w in sentence:
      indices = torch.cat(
          (indices, torch.Tensor([vocab[w]]).to('cpu')), dim=-1)

  return indices


def vocab_itos(data_index, vocab):
  words = [vocab[index] for index in data_index.tolist()]

  return words


def clean(sentence):
  blabla = [word for word in sentence if word != '<pad>']

  return blabla


def add_None(window_size, sentence):
  if len(sentence) is not window_size:
      sentence = sentence + ['<pad>'] * (window_size - len(sentence))

  return sentence


def PoStagger(window_size, sentence, vocab, tag_vocab):
  sentence = vocab_itos(sentence, vocab)
  sentence = clean(sentence)

  tags = [tag for word, tag in pos_tag(sentence)]
  tags = add_None(window_size, tags)
  tags_idx = encode_tag(tags, tag_vocab)
  return tags_idx


classifier_1_weights_path = "model/NER_classifier_1_weights.pth"
classifier_2_weights_path = "model/NER_classifier_2_weights.pth"

vocabulary_path = open("model/Words_Vocab.json")
glove_vocab = json.load(vocabulary_path)

label_vocabulary_path = open("model/label_Vocab.json")
label_vocab = json.load(label_vocabulary_path)

PosTag_Vocab_path = open("model/PoStag_Vocab.json")
PosTag_Vocab = json.load(PosTag_Vocab_path)

glove_vectors = torch.load('model/glove_vectors.pt', map_location='cpu')
glove = torch.nn.Embedding.from_pretrained(
  glove_vectors, padding_idx=0, freeze=True)


def build_model(device: 'cpu') -> Model:
  # STUDENT: return StudentModel()
  # STUDENT: your model MUST be loaded on the device "device" indicates
  return StudentModel()


'''
class RandomBaseline(Model):
options = [
  (3111, "B-CORP"),
  (3752, "B-CW"),
  (3571, "B-GRP"),
  (4799, "B-LOC"),
  (5397, "B-PER"),
  (2923, "B-PROD"),
  (3111, "I-CORP"),
  (6030, "I-CW"),
  (6467, "I-GRP"),
  (2751, "I-LOC"),
  (6141, "I-PER"),
  (1800, "I-PROD"),
  (203394, "O")
]

def __init__(self):
  self._options = [option[1] for option in self.options]
  self._weights = np.array([option[0] for option in self.options])
  self._weights = self._weights / self._weights.sum()

def predict(self, tokens: List[List[str]]) -> List[List[str]]:
  return [
      [str(np.random.choice(self._options, 1, p=self._weights)[0])
            for _x in x]
      for x in tokens
  ]
'''


class Hyperparam_tern():
  lstm_hid_size = 128
  nLayers = 2
  fc1_size = 10
  dropout = 0.3
  bidirectional = True


hparam_1 = Hyperparam_tern()


class HParams_2():
  lstm_hid_size = 200
  nLayers = 2
  fc1_size = 80
  fc2_size = 40
  # fc3_size = 20
  bidirectional = True


hparam_2 = HParams_2()


################################################################# Classifiers ############################################################

class Classifier_1(nn.Module):

  def __init__(self,
                hparams,
                glove
                ):
      super(Classifier_1, self).__init__()

      self.lstm = nn.LSTM(100, hparams.lstm_hid_size, hparams.nLayers, bidirectional=hparams.bidirectional,
                          device='cpu', dropout=hparams.dropout if hparams.nLayers > 1 else 0)
      self.lin1 = nn.Linear(hparams.lstm_hid_size * 2 if hparams.bidirectional is True else hparams.lstm_hid_size,
                            hparams.fc1_size)
      self.lin2 = nn.Linear(hparams.fc1_size, 3)
      self.drop = nn.Dropout(0.25)

  def forward(self, input):
      embedding = glove(input)
      x, (h, c) = self.lstm(embedding)
      x = self.drop(x)
      x = nn.Tanh()(self.lin1(x))
      logits = self.lin2(x)
      return logits


class Classifier_2(nn.Module):
  def __init__(self,
                hparams,
                glove,
                vocab,
                tag_vocab
                ):
      super(Classifier_2, self).__init__()

      self.vocab = vocab
      self.vocab_itos = list(vocab.keys())
      self.tag_vocab = tag_vocab
      self.lstm = nn.LSTM(100, hparams.lstm_hid_size, hparams.nLayers,
                          bidirectional=hparams.bidirectional, device='cpu')
      self.dropout1 = nn.Dropout(0.25)
      self.dropout2 = nn.Dropout(0.25)
      self.dropout3 = nn.Dropout(0.25)
      self.dropout4 = nn.Dropout(0.25)
      # self.lin = nn.Linear(hparams.lstm_hid_size*2+1, hparams.fc_size)
      self.lin1 = nn.Linear(hparams.lstm_hid_size * 2 if hparams.bidirectional is True else hparams.lstm_hid_size,
                            hparams.fc1_size)
      self.lin2 = nn.Linear(hparams.fc1_size, hparams.fc2_size)
      self.lin3 = nn.Linear(hparams.fc2_size + 3 + 1, 14)

      self.out = nn.Sigmoid()

  def forward(self, input):
      classifier_level_1 = Classifier_1(hparam_1, glove)
      classifier_level_1.load_state_dict(torch.load(
          classifier_1_weights_path, map_location=torch.device('cpu')))

      classifier_level_1.eval()
      with torch.no_grad():
          x1_logits = classifier_level_1(input)
          x1_logits = nn.Softmax(dim=-1)(x1_logits)

      x_tags = torch.FloatTensor([]).to('cpu')

      for sentence in input:
          ble = PoStagger(
              input.size()[-1], sentence, self.vocab_itos, self.tag_vocab).unsqueeze(0)
          x_tags = torch.cat((x_tags, ble), dim=0)
      x_tags = x_tags.unsqueeze(-1)

      embeddings = glove(input)

      x, (h, c) = self.lstm(embeddings)
      x = self.dropout1(x)

      x = nn.Tanh()(self.lin1(x))

      x = self.dropout2(x)

      x = nn.Tanh()(self.lin2(x))
      x = torch.cat((x1_logits, x), dim=-1)
      x = torch.cat((x_tags, x), dim=-1)

      return self.lin3(x)


class StudentModel(Model):

  def predict(self, tokens: List[List[str]]) -> List[List[str]]:
      data = Data2idx(tokens, glove_vocab, label_vocab)
      classifier_level_2 = Classifier_2(
          hparam_2, glove, vocab=glove_vocab, tag_vocab=PosTag_Vocab)

      classifier_level_2.load_state_dict(torch.load(
          classifier_2_weights_path, map_location=torch.device('cpu')))

      classifier_level_2.eval()
      labels = []
      with torch.no_grad():
          for sentence in data.OriginalData_encoded:
              logits = classifier_level_2(sentence['inputs'].unsqueeze(0))
              predictions = list(torch.argmax(logits, -1))
              label = data.label_decoder(predictions)
              labels.append(label)
      assert np.shape(tokens) == np.shape(labels)
      return labels


class Data2idx(Dataset):
  def __init__(self,
                input_data,
                word_vocab,
                label_vocab,
                device='cpu'):

      self.input_data = input_data
      self.device = device

      self.label2idx_dict = label_vocab
      self.label_itos = list(label_vocab.keys())

      self.word2idx_dict = word_vocab
      self.ternary_vocab = ternary_vocab = {'<pad>': 0, 'O': 1, 'NE': 2}
      self.OriginalData_encoded = None
      self.OriginalData_encoded_ter = None

      self.index_O_data(self.input_data)
      self.index_O_data_ter(self.input_data)

  def index_O_data(self, sentences):
      self.OriginalData_encoded = list()
      for i in range(len(sentences)):
          elem = sentences[i]
          encoded_elem = torch.LongTensor(
              self.encode_text(elem)).to(self.device)

          self.OriginalData_encoded.append({'inputs': encoded_elem})

  def index_O_data_ter(self, sentences):
      self.OriginalData_encoded_ter = list()
      for i in range(len(sentences)):
          elem = sentences[i]
          encoded_elem = torch.LongTensor(
              self.encode_text(elem)).to(self.device)

          self.OriginalData_encoded_ter.append({'inputs': encoded_elem})

  def encode_text(self, sentence):

      indices = list()
      for w in sentence:
          if w is None:
              indices.append(self.word2idx_dict['<pad>'])
          elif w in self.word2idx_dict:  # vocabulary string to integer
              indices.append(self.word2idx_dict[w])
          else:
              indices.append(self.word2idx_dict["<unk>"])
      return indices

  def encode_ternary_label(self, input):

      indices = list()
      for w in input:
          if w is None:
              indices.append(self.ternary_vocab['<pad>'])
          elif w in self.ternary_vocab:
              indices.append(self.ternary_vocab['O'])
          else:
              indices.append(self.ternary_vocab['NE'])
      return indices

  def label_decoder(self, label_idxs):
      labels = []
      for idx in list(label_idxs):
          for word in idx:
              labels.append(self.label_itos[word])
          

      return labels
